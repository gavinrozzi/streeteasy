---
title: Predicting Real Estate Prices in New York City Using Natural Language Processing and Machine Learning Algorithms
author:
    
  - name: Lauren Low
    email: llow@smith.edu
    affiliation: 1
  - name: Dayana Meza
    email: dmeza@smith.edu
    affiliation: 1
  - name: Emma Scott
    email: escott@smith.edu
    affiliation: 1
  - name: Xian (Elaine) Ye
    email: xye@smith.edu
    affiliation: 1
  - name: Yanwan Zhu
    email: yzhu62@smith.edu
    affiliation: 1
affiliation:
  - num: 1
    address: Smith College, 1 Chapin Way, Northampton, MA 01063

type: article
status: submit
bibliography: mybibfile.bib
appendix: appendix.tex
journal: data

abstract: |
  In real estate price prediction models, listings descriptions, usually entered by agents, may contain information that is not captured by the physical property attributes and potentially help differentiate the prices of houses that have similar features. This study uses the housing data of 41,235 properties in New York and New Jersey states that were listed on Streeteasy in 2019 and examines whether machine learning models with extracted features from text descriptions can improve the accuracy of house price prediction and outperform models with basic non-text-based features. Using keyword extraction and sentiment analysis, we find that the random forest model with text-based variables decreases MAPE from 27% to 25% and decreases RMSE from $400,027 to $381,930, compared to the model that only has non-text-based features. The most important text-based predictors include sentiment score, `master`, `marble`, `window`, and `private`. Error analysis shows our model overpredicts listing prices lower than $500,000 and underpredicts listing prices greater than $3,500,000. Our findings suggest that text-based features slightly improve model prediction.
keywords: |
  home price prediction; real esatate; random forest; natural language processing; text analysis; machine learning 
acknowledgement: |
  We thank Yipeng Lai ‘17 at StreetEasy for sponsoring this capstone project and providing inspiring discussions throughout this project. We thank Dr. Ben Baumer for being a supportive instructor and giving useful feedback on our progress at each stage of the project. 
conflictsofinterest: |
  The authors declare no conflict of interest.
output:
  bookdown::pdf_book:
    base_format: rticles::mdpi_article
header-includes:
  - \usepackage{longtable}
  - \usepackage{textcomp}
  - \usepackage{caption}
  - \usepackage{float}
  - \usepackage{booktabs}
---

```{r setup, include = FALSE}
library(tidyverse)
library(kableExtra)
library(bookdown)
setwd("/Users/zhuyanwan/Document/Smith/Spring 2021/SDS 410/streeteasy")
source("pre-processing.R")
load_data() 
clean_data()
sale_listings_imputed <- impute_data()
source("text_processing.R")
text_var()
```

# Introduction

The largest investment an individual makes over the course of their lifetime is often the purchase of real estate. Traditionally, home valuation has been conducted by professional property appraisers, those licensed and hired to give an opinion of a home's value, based on prices of neighboring homes, property analysis and judgment. In recent years, however, there has been an uptick in models and algorithms for estimating housing price. Companies like Zillow, Trulia, and others have developed machine learning models to automate the job of an appraiser. These organizations take into account housing attributes including property size, number of bedrooms, number of bathrooms, geographic location, state of the economy, among other variables to predict home prices.

StreetEasy, launched in 2006 and acquired by Zillow in 2013, is reshaping the way people buy, sell and rent property in New York and New Jersey. Similar to Zillow's Zestimate, StreetEasy's existing predictive tool uses metrics on bedrooms, bathrooms, property size, and geographic features to estimate particular listing prices. While these physical features of a property are certainly important to integrate in models to predict housing price, researchers have found that basic features alone are not enough to differentiate price for houses that have similar features, and the text descriptions may contain more details of a property that could potentially impact the price [e.g., @vargas2019model; @lawani2019reviews; @shen2021information].

## Related Work

Our interest is in using natural language processing (NLP) to explore whether text information informs the quality of a property and can be used to improve the prediction accuracy of real estate prices. Current studies have applied different NLP techniques, including `doc2vec`, word extraction, and sentiment analysis, to analyze text data, such as property descriptions or customers' reviews on short-term rentals, and used textual information to improve performances of machine learning models [@vargas2019model; @lawani2019reviews; @shen2021information]. @vargas2019model examine tokens in listing descriptions that are correlated with price (e.g., "swimming pool", "remodel") and find that the gradient boosted model with text descriptions based on `doc2vec` performed better than the one that only uses property features. Using sentiment analysis of customers' reviews on Airbnb as a proxy of listing quality, @lawani2019reviews extract the words associated with positive or negative emotions from the reviews and derive a sentiment score for each Boston Airbnb listing. Their results suggest that sentiment scores performed better at predicting host price than a single rating indicator that asks customers how satisfied they were using a scale from 1 to 5. @shen2021information extract "latent attributes" of textual descriptions and quantify the uniqueness of a property by using paragraph vectorization method that identifies unique words in a document; they show that incorporating information about the uniqueness of properties increased prediction accuracy of the housing price prediction model.

## Research Questions and Objectives

Considering that text features are able to increase prediction accuracy based on previous literature, a primary objective of this project is to convert text-based listing descriptions into input variables for the purpose of improving predictive model accuracy. In particular, we investigate whether listing descriptions can be utilized to create meaningful features to improve the accuracy of predicting home prices.

# Methods

## Data and Variables

The data provided by our project partner includes two datasets on sale listings and amenities. StreetEasy collects information from agents by having them directly enter listing information on the website or through a feed from their brokerage. StreetEasy verifies the listing and property information based on records from New York City's Department of Finance and Department of Buildings [@streeteasy_2019].

Our main data set consists of 59,661 sale listings of real estate properties listed and/or sold on StreetEasy in 2019. It has 31 variables including home price, property attributes (e.g., the number of bedrooms, the number of bathrooms, size in square feet), geospatial information (e.g., state, ZIP code, longitude, latitude), and a text description of each listing (see Table \ref{intial_var}).

## Data Preprocessing

We performed data cleansing and imputation to pre-process our data before running the analyses. First, we select the 95% quantile of the non-zero values as a reasonable range of the listing prices. We choose to do this to eliminate unreasonably high prices that are likely to be artifacts of data entry errors. For the same reason, we also filter out the outliers with unusually large values for property size and number of bedrooms and bathrooms.

In addition, we use ZIP codes to fill in estimated latitude and longitude values for observations without valid latitude/longitude information and create more geographical variables such as city, county, and state using the `zipcodeR` package that pulls information from U.S. Census data [@zipcodeR_2020]. We perform these two steps because of the varying quality of geospatial information in the original data set. First, some of the listings contain erroneous ZIP codes or longitude and latitude values such as (0, 0). Second, the original location information about areas and neighborhoods was entered manually by agents, so the location variables do not have uniform criteria and may differ in the level of specificity. Although the new geographic coordinates might not have street-level precision, by incorporating associated features from the `zipcodeR` package, we are able to obtain geographic information that is consistent across listings. Additionally, we group cities based on the median price within each city and create a new `city_group` variable with 10 levels [^1] (see Table \ref{citygroup}).

[^1]: We create the `city_group` variable because the original city variable contains too many levels to be handled by our random forest model using `caret` in R, which only allows categorical variables to have at most 32 levels.

Since some properties are associated with multiple listings in the original data, we remove the listings that appear more than once in the data set. The duplicate listings are results of agents posting the same listing multiple times with updated information. In order to eliminate duplicates, we first group the listings by `property_id` and keep the ones with the fewest missing values for all variables within each group. Next, we select the rows with the longest listing descriptions and the largest number of bedrooms within each group. Last, we use the `distinct()` function to keep one unique listing corresponding to each `property_id`.

Because nearly 20% of the observations do not have values for the `size_sqft` variable, we use the `norm.predict()` function in the `mice` package to impute missing property size values by regression [@mice_2011]. The predictors of the imputation consist of the number of bedrooms, the number of bathrooms, unit type, total floor count in the building, city, and state.

```{=tex}
\renewcommand{\arraystretch}{1.25}
\begin{table}[htb]
\caption{Summary statistics of numerical variables.}\label{numDescriptive}
\begin{center}
\begin{scriptsize} 
\begin{tabular} {l r r r r r r }
 \multicolumn{ 6 }{l}{ } \cr 
 \hline Variable  &   {n} &  {mean} &  {sd} &  {median} &  {min} &  {max}\cr 
  \hline 
price   &  41,325  &  1,125,462.75  &  874,401.83  &  829,000  &  40,000  &  4,599,000 \cr 
 bedrooms   &  41,325  &        2.58  &       2.11  &       2  &      0  &       36.00 \cr 
 bathrooms   &  41,325  &        1.85  &       1.13  &       2  &      0  &       20.00 \cr 
 size\_sqft   &  41,325  &     1520.31  &    1198.34  &    1190  &    100  &    64713.91 \cr 
 floor\_count   &  41,325  &        9.94  &      14.30  &       4  &      0  &       96.00 \cr 
 \hline 
\end{tabular}
\end{scriptsize}
\end{center}
\end{table}
```
```{=tex}
\renewcommand{\arraystretch}{1.25}
\begin{table}[ htpb ] 
\centering 
\caption{Summary statistics of categorical variables. More than 80\% of the data are listings in New York City, and the listings are mainly residential units. Please see Appendix A for information about the city\_group variable. }\label{catDescriptive}
\begin{tabular}{ l c }
\toprule
 &   \multicolumn{ 1 }{c}{ Count (\%) }\\ 
 & n = 41325 \\ 
 \midrule
county &  \\ 
\hspace{6pt}    Bergen County & 98 (0.2\%)\\ 
\hspace{6pt}    Bronx County & 1481 (3.6\%)\\ 
\hspace{6pt}    Essex County & 1 (0\%)\\ 
\hspace{6pt}    Hudson County & 6329 (15.3\%)\\ 
\hspace{6pt}    Kings County & 12008 (29.1\%)\\ 
\hspace{6pt}    Monmouth County & 1 (0\%)\\ 
\hspace{6pt}    Nassau County & 6 (0\%)\\ 
\hspace{6pt}    New York County & 12779 (30.9\%)\\ 
\hspace{6pt}    Ocean County & 4 (0\%)\\ 
\hspace{6pt}    Queens County & 7292 (17.6\%)\\ 
\hspace{6pt}    Richmond County & 1323 (3.2\%)\\ 
\hspace{6pt}    Union County & 3 (0\%)\\ 
unit type &  \\ 
\hspace{6pt}    CONDO & 17500 (42.3\%)\\ 
\hspace{6pt}    MULTIFAMILY & 5624 (13.6\%)\\ 
\hspace{6pt}    COOP & 9891 (23.9\%)\\ 
\hspace{6pt}    HOUSE & 6057 (14.7\%)\\ 
\hspace{6pt}    TOWNHOUSE & 1069 (2.6\%)\\
\hspace{6pt}    CONDOP & 375 (0.9\%)\\ 
\hspace{6pt}    BUILDING & 266 (0.6\%)\\ 
\hspace{6pt}    UNKNOWN & 183 (0.4\%)\\ 
\hspace{6pt}    LAND & 167 (0.4\%)\\ 
\hspace{6pt}    RENTAL & 64 (0.2\%)\\ 
\hspace{6pt}    UNCLASSIFIED & 55 (0.1\%)\\ 
\hspace{6pt}    COMMERCIAL & 26 (0.1\%)\\ 
\hspace{6pt}    FRACTIONAL & 25 (0.1\%)\\ 
\hspace{6pt}    APARTMENT & 17 (0\%)\\ 
\hspace{6pt}    AUCTION & 5 (0\%)\\
\hspace{6pt}    ANYHOUSE & 1 (0\%)\\ 
city group &  \\ 
\hspace{6pt}    1 & 12779 (30.9\%)\\
\hspace{6pt}    2 & 934 (2.3\%)\\ 
\hspace{6pt}    3 & 13033 (31.5\%)\\ 
\hspace{6pt}    4 & 680 (1.6\%)\\ 
\hspace{6pt}    5 & 2371 (5.7\%)\\ 
\hspace{6pt}    6 & 5509 (13.3\%)\\ 
\hspace{6pt}    7 & 2974 (7.2\%)\\ 
\hspace{6pt}    8 & 2923 (7.1\%)\\ 
\hspace{6pt}    9 & 119 (0.3\%)\\ 
\hspace{6pt}    10 & 3 (0\%)\\ 
\bottomrule

\end{tabular}
\end{table}
```
After data cleaning, we eventually have 41,325 listings in the data set. Table \ref{numDescriptive} shows the summary statistics of all numerical variables. The mean price is \$1,125,424, but the standard deviation is over \$870,000. The number of bedrooms ranges from 0 to 36 with a mean of 2.58 (SD = 2.11). The number of bathrooms ranges from 0 to 20 with a mean of 1.13 (SD = 1.13). The listings are located in buildings with 10 floors on average. The distribution of `size` has a mean of 1496 square feet but a large standard deviation (SD = 1120.27). Thus, a typical listing is a 2 bed/2 bath, 1190 $ft^2$ house that cost \$829,000.

Table \ref{catDescriptive} shows the summary statistics of all categorical variables that are not created from listing descriptions. We can see that 30.9% of the listings are located in the New York County (Manhattan), 29.1% are located in the Kings County (Brooklyn), and 17.6% are located in the Queens County (Queens). In total, about 84% of the listings are located in New York City, which consists of New York, Kings, Queens, Bronx and Richmond counties. The majority of the listings are residential units such as condominium, multifamily home, or housing cooperative unit.

```{r sampleListing, echo=FALSE}
sale_listings_ss %>%
  filter(property_id == 9193686) %>%
  select(id:unittype, bedrooms:size_sqft, -size_sqft_na, listing_description) %>%
  t() %>%
  knitr::kable(longtable = FALSE,
             format = "latex",
             label = "sampleListing",
             caption = "An example listing with its physical attributes and text description, transposed from a row in our data. It is a co-op in Manhattan that has 2 bedrooms, 1 bathroom, and 900 square feet. The closing price of this listing is \\$1,175,000.") %>%
  kableExtra::kable_styling(full_width = FALSE) %>%
  column_spec(1, border_left = T) %>%
  column_spec(2, width = "30em", border_right = T)
```

Table \@ref(tab:sampleListing) shows an example of one listing, which is originally a row in our data, but transposed into column form for formatting. The listing is a 2 bedroom/ 1 bathroom, 900 $ft^2$ co-op unit located in Manhattan (New York County) and it cost \$1,175,000.

## Data Analysis

### Random Forest Model

In order to determine whether adding text features as predictors will improve the prediction accuracy of house price prediction model, we first create a random forest regression model without text features as a baseline model. The predictor variables are listed and defined in Table \@ref(tab:noTextVar). We then compare the evaluation metrics of this model and the model with the extracted text features to see if incorporating text-based features improves prediction accuracy. We select the root mean squared error (RMSE), and the mean average percentage error (MAPE) as the evaluation metrics for our random forest regression models. To avoid overfitting and ensure the generalizability of our models, we perform 5-fold cross validation on all of our models. We also split 70% of the data into the training set, and 30% into the test set. We use the `caret` package to build our 100-tree random forest models [@caret2020].

To search the most optimized random forest model, we tune our model parameters and compare RMSE and $R^2$ for models with different numbers of `mtry`, the number of variables randomly sampled at each split. When considering all levels of categorical variables, we have 71 predictors in total after all text-based variables are added. The default of `mtry` for random forest regression models in the `caret` package is the total number of predictors divided by 3, so we test models with `mtry` from 20 to 35. The metrics suggest that the optimal `mtry` is 32, so we use `mtry = 32` for our models with and without text features.

```{r noTextVar, echo=FALSE}
# list of final non-text-based variables
var_no_text <- data.frame(Variable = c("bedrooms", 
                                 "bathrooms",
                                 "unittype", 
                                 "size_sqft",
                                 "is_historic",
                                 "floor_count",
                                 "county", 
                                 "city_group"), 
                    Definition = c("Number of bedrooms",
                             "Number of bathrooms",
                             "Property type",
                             "Size in square feet",
                             "Whether is in a historic building or not",
                             "Number of floors in the building",
                             "County",
                             "Grouping of cities based on average price"),
                    Notes = c("Numeric. Same as in original data",
                             "Numeric. Same as in original data",
                             "Categorical. Same as in original data.",
                             "Numeric. Imputed values (the imputation process is described in the  text). ",
                             "Binary (0 or 1). Same as in original data.",
                             "Numeric. Same as in original data.",
                             "Categorical. Inferred from zip codes",
                             "Categorical. Ten categories created based on median prices of properties in each city.")
                    )

knitr::kable(var_no_text, 
             longtable = FALSE, 
             format='latex',
             caption = 'List of predictor variables for the baseline model without text-based features.') %>%
  kableExtra::kable_styling(full_width = FALSE)%>%
  column_spec(1, width = "5em", border_left = T) %>%
  column_spec(2, width = "15em") %>%
  column_spec(3, width = "20em", border_right = T)%>%
  row_spec(0, bold=T)
```

### Text Analysis

To convert the listing descriptions into meaningful text-based features, we mainly use the bag-of-words (BoW) approach, which represents each listing description as a collection of words disregarding grammar. Each listing description is first tokenized, so that each text string is split into word tokens, and every list of tokens is vetted for stop words, so that words that have little to no meaningful information for analysis (e.g., 'the', 'as', 'a', 'of') are removed. Finally, we analyze the tokenized listing descriptions without stop words and extract features that may improve the prediction model, including binary variables and sentiment score.

To create binary variables based on text features, we use regular expressions to develop a text processing algorithm that reviews each listing description for keywords. For example, in order to extract information about washer/dryer in each listing, we use the following regular expressions to capture phrases related to washer/dryer:

        wd = as.integer(str_detect(listing_description, c(
              "[Ww]d",
              "[Ww]/d",
              "[Ww]&d",
              "[Ww] & d",
              "[Ww]asher/dryer",
              "[Ww]asher / dryer",
              "[Ww]asher and dryer",
              "[Ww]asher&dryer",
              "[Ww]asher & dryer",
              "[Ww]asher-dryer",
              "[Ww]asher dryer"))

If the term or its variants (i.e., hyphenated terms, capitalized terms, plural terms, etc.) are present in a listing description, the attribute for the description yields a 1 and if absent a 0. Using this method, we create 29 text-based binary variables (see Table \ref{binary} for summary statistics of these binary variables).

Furthermore, we investigate the listing descriptions using sentiment analysis, also known as opinion mining. While reading, humans are constantly interpreting and categorizing emotional intent like whether a text is generally positive or negative. Or, in a more complex analysis, a text might be characterized as antagonistic or hopeful, for example. Text mining approaches allow us to programmatically analyze the emotion of text. In order to perform these analyses, we view the listing descriptions as combinations of individual words where the sentiment content of the entire text is the sum of these word tokens.

We chose to use the `AFINN` lexicon developed by @nielsen2011new to aid the interpretation of emotional intent in text. The `AFINN` lexicon assigns each single word a score ranging from -5 to 5, where negative scores align with negative sentiments and positive scores with positive sentiments. `AFINN` is validated using combinations of crowdsourcing, individual labor by the author, or some combination of the two. It is important to note that neutral English words are oftentimes left out of lexicons, and qualifiers before a word like "no good" or "not true" are not corrected for reversed semantic meanings. We use `dplyr::inner_join()` to match the `AFINN` sentiment scores with our tokenized and stop-word-less listing descriptions. Then, we sum the score of the individual words to obtain one overall score per listing such that the `score` variable can be integrated into machine learning models. Table \@ref(tab:textVar) lists all text-based variables we use in the final model, including 29 binary variables and 1 numerical sentiment score variable.

To illustrate how text features are generated from listing descriptions, we can look into the text description of the example listing in Table \@ref(tab:sampleListing).

> Outstanding large 2 bedroom/1 bath apartment with *glorious* **views** of the **Hudson River**, has been *beautifully* **renovated** and features an open kitchen with **custom** cabinetry, Ceasar stone counter *tops*, Liebherr/Miele/Bosch appliances, Miele washer-dryer and Brazilian Cherry **hardwood floors**. The well proportioned bedrooms can accommodate king size beds and have ample **closet space**. This apartment is part of West Village Houses, a 42-building coop known for its *beautifully* landscaped common gardens providing an oasis from the hustle and bustle of the city. The coop is in *excellent* financial condition, has an on site management office, live in superintendent, bike rooms, laundry and they welcome your pets. This apartment is located on the fourth floor and affords direct **view** and *glorious* sunsets of Hudson River

This property description mentions "hardwood floor", "Hudson River", "custom", "view", "renovated", and "closet space", which yield values of 1 for related binary variables. Table \@ref(tab:samplescore) illustrates how a sentiment score is assigned to the example listing (Table \@ref(tab:sampleListing)). The example listing has a total sentiment score of 20, because it mentions "outstanding", "tops", "excellent", "beautifully" (twice), and "glorious" (twice).

```{r samplescore, echo=FALSE}
score_sample <- data.frame(listing_id = c(192, 192, 192, 192, 192, 192, 192),
           word = c("outstanding", "glorious", "beautifully", "tops", "beautifully", "excellent", "glorious"),
           values = c(5, 2, 3, 2, 3, 3, 2))

knitr::kable(score_sample, 
             longtable = FALSE,
             format='latex',
             caption = 'Sentiment scores assigned to words in the example listing description. This listing has a total sentiment score of 20 by mentioning the words in the table.')%>%
  kableExtra::kable_styling(full_width = FALSE)%>%
  column_spec(1, border_left = T) %>%
  column_spec(3, border_right = T)%>%
  row_spec(0, bold=T)
```

# Results

## Models without Text Features

In order to establish the baseline for model comparison, we build a 100-tree random forest model with 5-fold cross-validation to predict housing price using `bedroom`, `bathrooms`, `floor_count`, `is_historic`, `county`, `city_group` as predictors (see Table \@ref(tab:noTextVar) for variable definitions).

```{r improvement, echo=FALSE}
mape_rmse <- data.frame(Variables = c("bed, bath, 
                                       size in sqft",
                             "unittype",
                             "county",
                             "is_historic",
                             "floor_count",
                             "city_group",
                            # "stainless_steel, 
                            # hw_floors, wd, pet",
                             "sentiment score",
                             "all variables"),
                    MAPE = c(0.600,
                             0.515,
                             0.338,
                             0.329,
                             0.297,
                             0.270,
                           #  0.266,
                             0.260,
                             0.248
                             ),
                    RMSE = c(662274.4,
                             585639.2,
                             456559.4,
                             444032.5,
                             410053.5,
                             400027.5,
                             #396093.3,
                             390974,
                             381930.2
                             ))

knitr::kable(mape_rmse, 
             longtable = FALSE,
             format='latex',
             caption = 'Comparison of MAPE and RMSE when variables are sequentially added in 100-tree random forest models.')%>%
  kableExtra::kable_styling(full_width = FALSE)%>%
  column_spec(1, border_left = T) %>%
  column_spec(3, border_right = T)%>%
  row_spec(0, bold=T)
```

Table \@ref(tab:improvement) shows the improvement of prediction performance, indicated by the RMSE and MAPE metrics, when non-text-based variables are sequentially added into the model. The final model without text features yields a MAPE of 27% and an RMSE of \$400,027, meaning that the predicted price is on average 27% or more than \$400,000 off from the true price.

## Text Analysis

### Unigram and bigram analyses

We first perform n-gram analysis to examine the most frequent words and phrases in the listing descriptions (Figure \ref{fig:unigram} and \ref{fig:bigram}). The unigram analysis shows that among the most frequent words are bedroom, kitchen, home, and living. While these words are not particularly informative in creating binary variables, other words such as storage, private, and park are used to inform binary variable creation.

The bigram analysis reveals that the terms stainless steel, steel appliances, hardwood floors, washer and dryer, and fitness center, are some of the most frequently used phrases in this set of listing descriptions. Features related with a property's interior design, appliances and amenities are often included in the listing description, but are not reflected in the original data. Therefore, we want to incorporate into our model these property features that are common in descriptions and examine if they can improve the price prediction performance.

```{r unigram, fig.cap = "Top 50 most frequent words from unigram analysis. Among the most frequent words are bedroom, kitchen, home, and living.  While these words are not particularly informative in creating binary variables, other words such as storage, private, and park are used to inform binary variable creation.", out.width = "\\textwidth", echo=FALSE}
knitr::include_graphics("top50_word.pdf")
```

```{r bigram, fig.cap = "Top 50 most frequent two-word phrases from bigram analysis. Among the most frequent bigrams are stainless steel, hardwood floors, washer and dryer, steel appliances, and fitness center. These bigrams are used to inform binary variable creation.", out.width = "\\textwidth", echo=FALSE}
knitr::include_graphics("top50_bigram.pdf")
```

For further selection of keywords, we examine the relationship between frequency of words and bigrams and their associated average price. As we can see from Figure \ref{fig:wordprice}, *marble* is one of the top forty most frequently used words with the highest average associated price over \$3 million dollars. When analyzing bigrams, we find that *central park* is the most frequently used term with the highest average associated price at over \$4 million. Words and bigram phrases that are associated with relatively high prices, such as *marble*, *master*, *central park*, and *hudson river*, are used to inform binary variable creation.

```{r wordprice, fig.cap = "Top 40 frequent words or bigrams and the average price of the listings that mention these words. A lighter shade of blue indicates that the word or phrase is associated with a higher average number of bathrooms. A larger circle indicates that the word or phrase is associated with a higher average number of bedrooms. Words and phrases that are associated with relatively high prices are used to inform binary variable creation.", fig.show="hold", out.height = "45%", echo=FALSE}
knitr::include_graphics("word_price.pdf")
knitr::include_graphics("bigram_price.pdf")
```

## Random Forest Models with Text-based Variables

We add 29 newly created binary variables and the sentiment score variable to the 100-tree random forest model. Table \@ref(tab:textVar) lists all text-based variables we use in the optimal model. The introduction of binary variables and sentiment score changes the MAPE from 27% to 24.8%, decreasing the MAPE by approximately 7.4%. In other words, the random forest model becomes approximately 7.4% more accurate once text-based variables are included. The RMSE of the model with binary text-based variables is \$381,930, which means the predictions are on average \$20,000 more accurate for each listing.

```{r varImpPlot, fig.cap = "Variable importance plot of the optimal random forest model with text-based variables. Of the top twenty most important variables, size in square feet is most important with a score of 100. In general, text-based variables have importance scores less than 25 but do help improve model performance.", out.width = "\\textwidth", echo=FALSE}
knitr::include_graphics("varImpPlot.pdf")
```

Figure \ref{fig:varImpPlot} shows the relative importance of variables in the random forest model. The relative importance score on the x-axis is calculated by the mean square error on the out-of-bag data for each tree, and the differences are averaged and normalized by the standard error [@kuhn2012variable]. The importance score is scaled in a range of 0 to 100. The higher the value of the importance score, the higher the importance the variable is to the model. The variable importance plot allows us to select important features to include in our model, and steer us away from unimportant variables that are often not used in randomly sampled subsets of variables for each tree in the random forest.

As we can see from Figure \ref{fig:varImpPlot}, the square footage of a property is the most important variable in our model, followed by the number of bathrooms, city group, the county, and the number of floors in the building. The most important text-based variables are sentiment score, master, marble, window, private, renovate, and view. While the text-based variables do improve the model, they have relatively low importance compared to the physical features of the property. Nevertheless, we could infer from the importance score that the text-based variables contain more details of the properties, such as having marble decorations or renovation condition, which are not available in the non-text-based features.

## Sentimental Score and Word Count

Using a linear regression model, we find a positive relationship between sentiment score, the outcome variable, and word count of listing descriptions, the predictor variable. Assumption of equal variances or homoscedasticity is violated and therefore we should be cautious with interpreting the results. Figure \ref{fig:regression} shows a positive relationship between word count and sentiment score. In interpreting Table \ref{regressionTable}, because the p-value is smaller than 0.05 (p \< 0.001), we conclude that there is a statistically significant relationship between a listing description's word count and sentiment score. For every additional word included in a listing description, predicted sentiment score increases by 0.06. If there were no words in the listing description, the model predicts the sentiment score as 0.5. Both the model and the scatterplot show that longer listing descriptions are associated with a more positive sentiment score.

```{r regression, fig.cap = "Relationship between sentimental score and word count of listing descriptions. The scatterplot reveals that as word count increases, so does sentiment score as is reflected by the blue trend line.", out.width = "\\textwidth", echo=FALSE}
knitr::include_graphics("scatterplot.pdf")
```

```{=tex}
\begin{table}[htbp] \centering 
  \caption{Regression Output of Word Count and Sentiment Score. In this regression table, p < 0.001 which is less than the threshold value of 0.05, indicating a significant relationship between listing descriptions’ word count and sentiment score.} 
  \label{regressionTable} 
\begin{tabular}{@{\extracolsep{5pt}}lc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{1}{c}{\textit{Outcome variable:}} \\ 
\cline{2-2} 
\\[-1.8ex] & score \\ 
\hline \\[-1.8ex] 
 word\_count & 0.062$^{***}$ \\ 
  & (0.0003) \\ 
  & \\ 
 Intercept & 0.519$^{*}$ \\ 
  & (0.063) \\ 
  & \\ 
\hline \\[-1.8ex] 
Observations & 41,325 \\ 
R$^{2}$ & 0.510 \\ 
Adjusted R$^{2}$ & 0.510 \\ 
Residual Std. Error & 7.257 (df = 41323) \\ 
F Statistic & 43,011.900$^{***}$ (df = 1; 41323) \\ 
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{1}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
\end{tabular} 
\end{table} 
```
## Error Analysis

```{r plotError, fig.cap = "Relationship between home prices and prediction errors greater than third quartile. This plot only includes the least accurate predictions of test data, which overestimate or underestimate the listing prices by more than \\$301,749. On the x-axis is the listing price and on the y-axis is the prediction error calculated by subtracting the actual price from the predicted price. The grey vertical line is the mean listing price in the test data. The plot shows that listing prices greater than \\$3,500,000 always get underestimated and listing prices lower than \\$500,000 always get overestimated.", out.width = "\\textwidth", echo=FALSE}
knitr::include_graphics("error_3rd.pdf")
```

To further assess the model performance, we analyze out-of-sample errors, which are prediction errors generated when we test our model on the test data. Figure \ref{fig:plotError} shows the relationship between prediction error (only greater than the third quartile) and actual home price for the least accurate predictions yielded by our model. We can see that our model tends to overpredict below average listing prices and substantially underpredict home prices greater than \$3,500,000. For the upper tail of the price distribution, the error could be as large as more than half of the actual home price, suggesting that our model has poor performances in accommodating outliers with extremely high prices. Similarly, for listings that have below average prices, our model could predict the listing price to be 3 to 4 times higher than the actual price, suggesting that our model may also perform poorly for listings at the lower tail of the price distribution.

# Discussion

## Conclusion

This study examines whether text features from property descriptions could improve prediction accuracy of housing price. Using keyword extraction and sentiment analysis, we find that the random forest model with text features is 2.2 percent point, or 7.4% more accurate than the model without text features and decreases the RMSE by \$20,000. Specifically, sentiment score, `master,` `marble`, `window`, `private`, `offer`, `renovate`, and `view` are the top important text features that improve the model. The results are consistent with previous literature, which shows that sentiment analysis and unique features could improve the prediction of housing price. The binary variables indicate additional features of the property that are not available in property attributes and thus increase prediction accuracy. For instance, `master` might indicate `master bedroom` or `master suite`; `marble` suggests the material of the bathroom; and `view` might indicate the property has good views of the city.

Our error analysis suggests that our best model underestimates the houses that have extremely high prices, and those errors may result in the large RMSE. A possible explanation for the underestimated price is that we do not have predictors specific to listings with high values. Our keyword extraction method relies on the most frequent words, which might not capture the unique attributes of houses with high values. To improve the text analysis in the future, it will be important to extract text features that have discriminability for home prices, especially extremely high-priced listings.

## Limitations

Some limitations of our analyses are worth noting. First, our approach to extract housing features treats words as isolated. For example, it assumes that if a listing contains keywords such as "renovate" and its variants, it should have been renovated. However, the keyword extraction method disregards the context of these keywords in that we are not sure if the text description mentions "completed renovation" or only "a plan for renovation". Second, we rely on the `AFINN` lexicon to assign scores to the sentiment-related words, but the lexicon is not specifically designed for housing descriptions and may thus assign unjustified scores to certain words. For example, although "block" and "lobby" are neutral descriptions of the listings, they are assigned negative scores by the `AFINN` lexicon. In addition, we are unable to represent the structural relations between words, such as in negative sentences, in the text. For instance, "miss" has a negative score in sentiment analysis but it often appears as "don't miss [this property]" which is actually a positive tone. Our text processing methods do not capture the reversed semantic meaning or reversed sentiments introduced by negation in sentences. Third, due to our limited computation resources, we only use 100-tree random forest models with 5-fold cross-validation, while in comparison, the default `caret::train(method = rf)` method uses 500 trees. This could limit the predictive accuracy and robustness of our models. Lastly, our model performance is limited by the relatively small size of the dataset, lack of precision in geospatial variables, and large amount of missing values in the square footage of listings.

## Future Directions

In this study, we use keyword extraction and sentiment analysis to improve price prediction machine learning model, yet previous literature has employed more advanced NLP techniques that could potentially improve model performance [e.g., @vargas2019model]. Future studies can explore the applications of `word2vec`, `doc2vec`, or other NLP techniques in predicting housing prices. Additionally, although it is not the scope of this project, given that housing prices could vary drastically within blocks, especially in New York, the price prediction model will benefit from incorporating a more precise geographic variable. Furthermore, including features such as the distance to local landmarks (e.g., Central Park, Hudson River) may also improve prediction accuracy.

## Ethics Statement

There are three primary avenues for ethical consideration in our work with StreetEasy: data collection, modeling and application of the model (see Appendix B for the full Ethics Statement). Our dataset might reproduce real-world inequalities by oversampling listings in high-income and predominantly white neighborhoods in the way that some listings are over-represented (like penthouses on the Upper East Side) or underrepresented, as shown in our data that Nassau County, Ocean County, Union County, Monmouth County, and Essex County all have less than 10 listings. Furthermore, our method of extracting text-based features may put an disadvantage on certain groups of users, including speakers of languages other than English or users with lower socioeconomic status.
